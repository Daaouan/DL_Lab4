# Lab 4: Deep Learning Architectures

## Introduction
This lab is designed to enhance our understanding and practical skills in creating and training deep neural networks using PyTorch, focusing on Auto-encoders (AE), Variational Auto-encoders (VAE), and Generative Adversarial Networks (GANs).

## Objectives
- Familiarize ourselves with the PyTorch library.
- Build and train deep neural network architectures for AE, VAE, and GANs.
- Understand the underlying mechanics and applications of generative AI technologies.

## Dataset
- MNIST Dataset for AE and VAE: [MNIST Dataset](https://www.kaggle.com/datasets/hojjatk/mnist-dataset)
- Abstract Art Gallery for GANs: [Abstract Art Gallery Dataset](https://www.kaggle.com/datasets/bryanb/abstract-art-gallery)

## Tasks
### Part 1: AE and VAE
1. Develop and train an auto-encoder architecture on the MNIST dataset.
2. Develop and train a variational auto-encoder architecture on the MNIST dataset.
3. Evaluate and analyze both models (e.g., Loss, KL divergence).
4. Visualize the latent spaces of the AE and VAE models.

### Part 2: GANs
1. Set up and train GAN architectures using the Abstract Art Gallery dataset.
2. Analyze the performance of both the Generator and Discriminator.
3. Generate new data and assess its quality compared to original data.

## Evaluation
At the end of the lab, we are expected to:
- Provide a synthesis of our learning outcomes.
- Push our work to a GitHub repository and draft a detailed README to summarize the lab's insights.

## Tools and Environment
- Google Colab or Kaggle for implementing and running the models.
- GitHub or GitLab for version control and submission.

## Conclusion
This lab is aimed at providing hands-on experience with advanced deep learning models, empowering us with the skills needed to design, implement, and evaluate generative models in PyTorch.

